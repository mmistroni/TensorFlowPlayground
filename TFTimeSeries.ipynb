{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFTimeSeries.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmistroni/TensorFlowPlayground/blob/master/TFTimeSeries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "03Y2LC7ZQtOu",
        "colab_type": "code",
        "outputId": "f40536b9-1610-4605-e254-f09221201db3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4251
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.contrib.learn import ModeKeys\n",
        "import tensorflow.contrib.rnn as rnn\n",
        "from datetime import datetime\n",
        "\n",
        "#tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "#TIMESERIES_COL = '9'\n",
        "TIMESERIES_COL = 'rawdata'\n",
        "N_OUTPUTS = 2  # in each sequence, 1-8 are features, and 9-10 is label\n",
        "SEQ_LEN = 10\n",
        "DEFAULTS = 0.0\n",
        "N_INPUTS = SEQ_LEN - N_OUTPUTS\n",
        "BATCH_SIZE = 20\n",
        "ROOT_DIR = '/home/mmistroni/tf_logs/rnn-run-{}'\n",
        "\n",
        "def create_time_series():\n",
        "  freq = (np.random.random()*0.5) + 0.1  # 0.1 to 0.6\n",
        "  ampl = np.random.random() + 0.5  # 0.5 to 1.5\n",
        "  x = np.sin(np.arange(0,SEQ_LEN) * freq) * ampl\n",
        "  return x\n",
        "\n",
        "all_timeseries = [create_time_series() for i in range(0, SEQ_LEN * 4)]\n",
        "\n",
        "\n",
        "# We need to stack X numpy array on top of each other and then create a dictionary\n",
        "# for every features\n",
        "column_names = [str(idx) for idx in range(0, SEQ_LEN)]\n",
        "\n",
        "feature_names = column_names[0:-N_OUTPUTS]\n",
        "labels = column_names[-N_OUTPUTS:]\n",
        "all_data = np.stack(all_timeseries)\n",
        "\n",
        "print('All data shape is{0}'.format(all_data.shape))\n",
        "X, y = all_data[...,0:-N_OUTPUTS], all_data[...,-N_OUTPUTS:]\n",
        "\n",
        "print ('X is fo type {0}, y  of type {1}'.format(type(X[0][0]), type(y)))\n",
        "\n",
        "\n",
        "print ('X.shape is {0}, y shap is {1}'.format(X.shape, y.shape))\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=1)\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "features_train = dict((fn,X_train[:, [idx]]) for idx, fn in enumerate(feature_names))\n",
        "labels_train = y_train\n",
        "\n",
        "\n",
        "# Test data\n",
        "features_test = dict((fn,X_test[:, [idx]]) for idx, fn in enumerate(feature_names))\n",
        "labels_test = y_test\n",
        "\n",
        "\n",
        "\n",
        "LSTM_SIZE = 3  # number of hidden layers in each of the LSTM cells\n",
        "\n",
        "# create the inference model\n",
        "def simple_rnn(features, labels, mode, params):\n",
        "  # 0. Reformat input shape to become a sequence\n",
        "  print ('IN Features are:{0}'.format(features))\n",
        "  x = tf.split(features[TIMESERIES_COL], N_INPUTS, 1)\n",
        "  #print 'x={}'.format(x)\n",
        "    \n",
        "  # 1. configure the RNN\n",
        "  lstm_cell = rnn.BasicLSTMCell(LSTM_SIZE, forget_bias=1.0)\n",
        "  outputs, _ = tf.nn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
        "\n",
        "  # slice to keep only the last cell of the RNN\n",
        "  outputs = outputs[-1]\n",
        "  #print 'last outputs={}'.format(outputs)\n",
        "  \n",
        "  # output is result of linear activation of last layer of RNN\n",
        "  weight = tf.Variable(tf.random_normal([LSTM_SIZE, N_OUTPUTS]))\n",
        "  bias = tf.Variable(tf.random_normal([N_OUTPUTS]))\n",
        "  predictions = tf.matmul(outputs, weight) + bias\n",
        "    \n",
        "  # 2. loss function, training/eval ops\n",
        "  if mode == ModeKeys.TRAIN or mode == ModeKeys.EVAL:\n",
        "     loss = tf.losses.mean_squared_error(labels, predictions)\n",
        "     train_op = tf.contrib.layers.optimize_loss(\n",
        "         loss=loss,\n",
        "         global_step=tf.train.get_global_step(),\n",
        "         learning_rate=0.01,\n",
        "         optimizer=\"SGD\")\n",
        "     eval_metric_ops = {\n",
        "      \"rmse\": tf.metrics.root_mean_squared_error(labels, predictions)\n",
        "     }\n",
        "  else:\n",
        "     loss = None\n",
        "     train_op = None\n",
        "     eval_metric_ops = None\n",
        "  \n",
        "  # 3. Create predictions\n",
        "  predictions_dict = {\"predicted\": predictions}\n",
        "\n",
        "  # 4. Create export outputs  \n",
        "  export_outputs = {\"predicted\": tf.estimator.export.PredictOutput(predictions)}\n",
        "\n",
        "  # 5. return ModelFnOps\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode=mode,\n",
        "      predictions=predictions_dict,\n",
        "      loss=loss,\n",
        "      train_op=train_op,\n",
        "      eval_metric_ops=eval_metric_ops,\n",
        "      export_outputs=export_outputs)\n",
        "\n",
        "def serving_input_receiver_fn():\n",
        "  feature_placeholders = {\n",
        "    TIMESERIES_COL: tf.placeholder(tf.float32, [None, N_INPUTS])\n",
        "  }\n",
        "\n",
        "  features = {\n",
        "    key: tf.expand_dims(tensor, -1)\n",
        "    for key, tensor in feature_placeholders.items()\n",
        "  }\n",
        "\n",
        "  features[TIMESERIES_COL] = tf.squeeze(features[TIMESERIES_COL], axis=[2], name='timeseries')\n",
        "\n",
        "\n",
        "\n",
        "# Creating a TrainFn and a TestFn\n",
        "def _train_fn(X, y, batch_size):\n",
        "    \n",
        "    def _train():\n",
        "        \"\"\"An input function for training\"\"\"\n",
        "        # Convert the inputs to a Dataset.\n",
        "        # TODO need to be refactored according to https://medium.com/google-cloud/how-to-do-time-series-prediction-using-rnns-and-tensorflow-and-cloud-ml-engine-2ad2eeb189e8\n",
        "        # this is not good.\n",
        "        X_32 = tf.cast(X, tf.float32)\n",
        "        y_32 = tf.cast(y, tf.float32)\n",
        "        inputs = tf.concat(X_32, axis=1)\n",
        "        label = tf.concat(y_32, axis=1)\n",
        "        features, labels = {TIMESERIES_COL: inputs}, label\n",
        "        \n",
        "        return features, labels\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(features, labels)\n",
        "        # Shuffle, repeat, and batch the examples.\n",
        "        dataset = dataset.repeat(None).batch(batch_size)\n",
        "        # This will now return batches of features, label\n",
        "        return dataset.make_one_shot_iterator().get_next()\n",
        "    return _train\n",
        "\n",
        "def _test_fn(X, y, batch_size):\n",
        "    def _test():\n",
        "        \"\"\"An input function for training\"\"\"\n",
        "        # Convert the inputs to a Dataset.\n",
        "        X_32 = tf.cast(X, tf.float32)\n",
        "        y_32 = tf.cast(y, tf.float32)\n",
        "        inputs = tf.concat(X_32, axis=1)\n",
        "        label = tf.concat(y_32, axis=1)\n",
        "        \n",
        "        features, labels = {TIMESERIES_COL: inputs}, label\n",
        "        \n",
        "        return features, labels\n",
        "        \n",
        "        dataset = tf.data.Dataset.from_tensor_slices(features, labels)\n",
        "        # Shuffle, repeat, and batch the examples.\n",
        "        dataset = dataset.repeat(1).batch(batch_size)\n",
        "        # This will now return batches of features, label\n",
        "        return dataset.make_one_shot_iterator().get_next()\n",
        "    return _test  \n",
        "  \n",
        " \n",
        "def experiment_fn(output_dir):\n",
        "    # run experiment\n",
        "    train_spec = tf.estimator.TrainSpec(\n",
        "          input_fn=_train_fn(X_train, y_train, BATCH_SIZE), max_steps=1000)\n",
        "    exporter = tf.estimator.FinalExporter('timeseries',\n",
        "    serving_input_receiver_fn)\n",
        "    eval_spec = tf.estimator.EvalSpec(\n",
        "            input_fn=_test_fn(X_test, y_test, BATCH_SIZE), \n",
        "            exporters=[exporter])\n",
        "    \n",
        "    estimator = tf.estimator.Estimator(model_fn=simple_rnn, model_dir=output_dir)\n",
        "\n",
        "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)    \n",
        "    \n",
        "output_dir = ROOT_DIR.format(datetime.utcnow().strftime('%Y%m%d%H%M%S'))  \n",
        "  \n",
        "experiment_fn(output_dir)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All data shape is(40, 10)\n",
            "X is fo type <class 'numpy.float64'>, y  of type <class 'numpy.ndarray'>\n",
            "X.shape is (40, 8), y shap is (40, 2)\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/home/mmistroni/tf_logs/rnn-run-20181119212712', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe29620e4e0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function simple_rnn at 0x7fe296bb0048>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "IN Features are:{'rawdata': <tf.Tensor 'concat:0' shape=(32, 8) dtype=float32>}\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /home/mmistroni/tf_logs/rnn-run-20181119212712/model.ckpt.\n",
            "INFO:tensorflow:loss = 1.1238475, step = 1\n",
            "INFO:tensorflow:global_step/sec: 391.797\n",
            "INFO:tensorflow:loss = 0.57500565, step = 101 (0.257 sec)\n",
            "INFO:tensorflow:global_step/sec: 826.332\n",
            "INFO:tensorflow:loss = 0.4430828, step = 201 (0.124 sec)\n",
            "INFO:tensorflow:global_step/sec: 841.685\n",
            "INFO:tensorflow:loss = 0.3324059, step = 301 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 847.695\n",
            "INFO:tensorflow:loss = 0.24222407, step = 401 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 824.189\n",
            "INFO:tensorflow:loss = 0.17196482, step = 501 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 847.79\n",
            "INFO:tensorflow:loss = 0.12028675, step = 601 (0.118 sec)\n",
            "INFO:tensorflow:global_step/sec: 839.299\n",
            "INFO:tensorflow:loss = 0.0841438, step = 701 (0.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 814.117\n",
            "INFO:tensorflow:loss = 0.059629276, step = 801 (0.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 837.402\n",
            "INFO:tensorflow:loss = 0.04321081, step = 901 (0.122 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into /home/mmistroni/tf_logs/rnn-run-20181119212712/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "IN Features are:{'rawdata': <tf.Tensor 'concat:0' shape=(8, 8) dtype=float32>}\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-11-19-21:27:15\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /home/mmistroni/tf_logs/rnn-run-20181119212712/model.ckpt-1000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Evaluation [50/100]\n",
            "INFO:tensorflow:Evaluation [60/100]\n",
            "INFO:tensorflow:Evaluation [70/100]\n",
            "INFO:tensorflow:Evaluation [80/100]\n",
            "INFO:tensorflow:Evaluation [90/100]\n",
            "INFO:tensorflow:Evaluation [100/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-11-19-21:27:16\n",
            "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 0.08562428, rmse = 0.29261628\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: /home/mmistroni/tf_logs/rnn-run-20181119212712/model.ckpt-1000\n",
            "INFO:tensorflow:Performing the final export in the end of training.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-fc295796b2fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mROOT_DIR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y%m%d%H%M%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m \u001b[0mexperiment_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-fc295796b2fc>\u001b[0m in \u001b[0;36mexperiment_fn\u001b[0;34m(output_dir)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimple_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(estimator, train_spec, eval_spec)\u001b[0m\n\u001b[1;32m    469\u001b[0m         '(with task id 0).  Given task id {}'.format(config.task_id))\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    608\u001b[0m         config.task_type != run_config_lib.TaskType.EVALUATOR):\n\u001b[1;32m    609\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running training and evaluation locally (non-distributed).'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;31m# Distributed case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mrun_local\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m         saving_listeners=saving_listeners)\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m     eval_result = listener_for_eval.eval_result or _EvalResult(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1205\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1239\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1240\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1469\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exception_type, exception_value, traceback)\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mexception_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m     \u001b[0;31m# __exit__ should return True to suppress an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mexception_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m_close_internal\u001b[0;34m(self, exception_type)\u001b[0m\n\u001b[1;32m    814\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m           \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/basic_session_run_hooks.py\u001b[0m in \u001b[0;36mend\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0mlast_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_step_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlast_step\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_triggered_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listeners\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m       \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/basic_session_run_hooks.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self, session, step)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0mshould_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listeners\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         logging.info(\n\u001b[1;32m    609\u001b[0m             \u001b[0;34m\"A CheckpointSaverListener requested that training be stopped. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mafter_save\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    515\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_trigger_for_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step_value\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# updates self.eval_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_continuous_eval_listener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         logging.info('Exiting evaluation, as requested by '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, global_step_value)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_last_triggered_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     self.eval_result, self.export_results = (\n\u001b[0;32m--> 537\u001b[0;31m         self._evaluator.evaluate_and_export())\n\u001b[0m\u001b[1;32m    538\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_EvalStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEVALUATED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m       \u001b[0;31m#  This is unexpected; should never happen.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mevaluate_and_export\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    922\u001b[0m           self._max_training_steps if self._max_training_steps else False)\n\u001b[1;32m    923\u001b[0m       export_results = self._export_eval_result(eval_result,\n\u001b[0;32m--> 924\u001b[0;31m                                                 is_the_final_export)\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mis_the_final_export\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36m_export_eval_result\u001b[0;34m(self, eval_result, is_the_final_export)\u001b[0m\n\u001b[1;32m    955\u001b[0m                 \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m                 \u001b[0meval_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m                 is_the_final_export=is_the_final_export))\n\u001b[0m\u001b[1;32m    958\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mexport_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/exporter.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, estimator, export_path, checkpoint_path, eval_result, is_the_final_export)\u001b[0m\n\u001b[1;32m    416\u001b[0m     return self._saved_model_exporter.export(estimator, export_path,\n\u001b[1;32m    417\u001b[0m                                              \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                                              is_the_final_export)\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/exporter.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mas_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         strip_default_attrs=self._strip_default_attrs)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mexport_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mexport_savedmodel\u001b[0;34m(self, export_dir_base, serving_input_receiver_fn, assets_extra, as_text, checkpoint_path, strip_default_attrs)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mstrip_default_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrip_default_attrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m         mode=model_fn_lib.ModeKeys.PREDICT)\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m   def export_saved_model(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_export_saved_model_for_mode\u001b[0;34m(self, export_dir_base, input_receiver_fn, assets_extra, as_text, checkpoint_path, strip_default_attrs, mode)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mas_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         strip_default_attrs=strip_default_attrs)\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m   def _export_all_saved_models(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_export_all_saved_models\u001b[0;34m(self, export_dir_base, input_receiver_fn_map, assets_extra, as_text, checkpoint_path, strip_default_attrs)\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0mbuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_receiver_fn_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m             \u001b[0mstrip_default_attrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_variables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m             mode=model_fn_lib.ModeKeys.PREDICT)\n\u001b[0m\u001b[1;32m    908\u001b[0m         \u001b[0msave_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_add_meta_graph_for_mode\u001b[0;34m(self, builder, input_receiver_fn_map, checkpoint_path, strip_default_attrs, save_variables, mode, export_tags, check_variables)\u001b[0m\n\u001b[1;32m    979\u001b[0m       \u001b[0;31m# Call the model_fn and collect the export_outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 981\u001b[0;31m           \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_receiver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m           \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_receiver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'labels'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m           \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'features'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "prI5nJS_S4nf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Installing libraries and downloading data as DataFrame\n"
      ]
    },
    {
      "metadata": {
        "id": "zwQN5CmUS_pS",
        "colab_type": "code",
        "outputId": "57d455cc-65a7-4d83-891a-0b03d3ba9d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pandas_datareader"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pandas_datareader\n",
            "  Using cached https://files.pythonhosted.org/packages/cc/5c/ea5b6dcfd0f55c5fb1e37fb45335ec01cceca199b8a79339137f5ed269e0/pandas_datareader-0.7.0-py2.py3-none-any.whl\n",
            "Collecting lxml (from pandas_datareader)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/a4/9eea8035fc7c7670e5eab97f34ff2ef0ddd78a491bf96df5accedb0e63f5/lxml-4.2.5-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.8MB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from pandas_datareader) (1.10.11)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from pandas_datareader) (2.18.4)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.6/dist-packages (from pandas_datareader) (0.22.0)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas_datareader) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas_datareader) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas_datareader) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas_datareader) (2018.10.15)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.2->pandas_datareader) (2018.7)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.2->pandas_datareader) (1.14.6)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.2->pandas_datareader) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas>=0.19.2->pandas_datareader) (1.11.0)\n",
            "Installing collected packages: lxml, pandas-datareader\n",
            "Successfully installed lxml-4.2.5 pandas-datareader-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZFt1ZH-GTPV-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas_datareader as pdr\n",
        "from datetime import date, timedelta\n",
        "startdate = date.today() - timedelta(days=100)\n",
        "stock_data = pdr.get_data_yahoo('^GSPC', startdate, date.today())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rIAYARAcTXzf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creating 10 batches of 10 Items"
      ]
    },
    {
      "metadata": {
        "id": "QeBP_1ykTgeP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "closes = stock_data[['Close']]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WafXje98Qyiq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def init(hparams):\n",
        "    global SEQ_LEN, DEFAULTS, N_INPUTS\n",
        "    N_OUTPUTS = 3  # in each sequence, 1-9 are features, and 10 is label\n",
        "    SEQ_LEN = 10\n",
        "    DEFAULTS = 0.0\n",
        "    DEFAULTS = [[0.0] for x in range(0, SEQ_LEN)]\n",
        "    N_INPUTS = SEQ_LEN - N_OUTPUTS\n",
        "\n",
        "\n",
        "def linear_model(features, mode, params):\n",
        "    X = features[TIMESERIES_COL]\n",
        "    predictions = tf.layers.dense(X, 1, activation=None)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def dnn_model(features, mode, params):\n",
        "    X = features[TIMESERIES_COL]\n",
        "    h1 = tf.layers.dense(X, 10, activation=tf.nn.relu)\n",
        "    h2 = tf.layers.dense(h1, 3, activation=tf.nn.relu)\n",
        "    predictions = tf.layers.dense(h2, 1, activation=None)  # linear output: regression\n",
        "    return predictions\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t3E-or7HQ2RF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cnn_model(features, mode, params):\n",
        "    X = tf.reshape(features[TIMESERIES_COL],\n",
        "                   [-1, N_INPUTS, 1])  # as a 1D \"sequence\" with only one time-series observation (height)\n",
        "    c1 = tf.layers.conv1d(X, filters=N_INPUTS // 2,\n",
        "                          kernel_size=3, strides=1,\n",
        "                          padding='same', activation=tf.nn.relu)\n",
        "    p1 = tf.layers.max_pooling1d(c1, pool_size=2, strides=2)\n",
        "\n",
        "    c2 = tf.layers.conv1d(p1, filters=N_INPUTS // 2,\n",
        "                          kernel_size=3, strides=1,\n",
        "                          padding='same', activation=tf.nn.relu)\n",
        "    p2 = tf.layers.max_pooling1d(c2, pool_size=2, strides=2)\n",
        "\n",
        "    outlen = p2.shape[1] * p2.shape[2]\n",
        "    c2flat = tf.reshape(p2, [-1, outlen])\n",
        "    h1 = tf.layers.dense(c2flat, 3, activation=tf.nn.relu)\n",
        "    predictions = tf.layers.dense(h1, 1, activation=None)  # linear output: regression\n",
        "    return predictions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "01uU1JjqQ5tC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rnn_model(features, mode, params):\n",
        "    CELL_SIZE = N_INPUTS // 3  # size of the internal state in each of the cells\n",
        "\n",
        "    # 1. dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
        "    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
        "\n",
        "    # 2. configure the RNN\n",
        "    cell = tf.nn.rnn_cell.GRUCell(CELL_SIZE)\n",
        "    outputs, state = tf.nn.dynamic_rnn(cell, x, dtype=tf.float64)\n",
        "\n",
        "    # 3. pass rnn output through a dense layer\n",
        "    h1 = tf.layers.dense(state, N_INPUTS // 2, activation=tf.nn.relu)\n",
        "    predictions = tf.layers.dense(h1, 1, activation=None)  # (?, 1)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# 2-layer RNN\n",
        "def rnn2_model(features, mode, params):\n",
        "    # dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
        "    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
        "\n",
        "    # 2. configure the RNN\n",
        "    cell1 = tf.nn.rnn_cell.GRUCell(N_INPUTS * 2)\n",
        "    cell2 = tf.nn.rnn_cell.GRUCell(N_INPUTS // 2)\n",
        "    cells = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
        "    outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
        "    # 'state' is now a tuple containing the final state of each cell layer\n",
        "    # we use state[1] below to extract the final state of the final layer\n",
        "    \n",
        "    # 3. pass rnn output through a dense layer\n",
        "    h1 = tf.layers.dense(state[1], cells.output_size // 2, activation=tf.nn.relu)\n",
        "    predictions = tf.layers.dense(h1, 1, activation=None)  # (?, 1)\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "34GMRROIQ9wJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rnnN_model(features, mode, params):\n",
        "    # dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
        "    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
        "\n",
        "    # 2. configure the RNN\n",
        "    cell1 = tf.nn.rnn_cell.GRUCell(N_INPUTS * 2)\n",
        "    cell2 = tf.nn.rnn_cell.GRUCell(N_INPUTS // 2)\n",
        "    cells = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
        "    outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
        "    # 'outputs' contains the state of the final layer for every time step\n",
        "    # not just the last time step (?,N_INPUTS, final cell size)\n",
        "    \n",
        "    # 3. pass state for each time step through a DNN, to get a prediction\n",
        "    # for each time step \n",
        "    h1 = tf.layers.dense(outputs, cells.output_size, activation=tf.nn.relu)\n",
        "    h2 = tf.layers.dense(h1, cells.output_size // 2, activation=tf.nn.relu)\n",
        "    predictions = tf.layers.dense(h2, 1, activation=None)  # (?, N_INPUTS, 1)\n",
        "    predictions = tf.reshape(predictions, [-1, N_INPUTS])\n",
        "    return predictions # return prediction for each time step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zDTmg52wRDTO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# read data and convert to needed format\n",
        "def read_dataset(filename, mode, batch_size=10):\n",
        "    # What we  need to do here is to somehow pass a 10*10 dataset. and we should not use decode_csv but rather do similar\n",
        "    # to what we have done in the california dtaset\n",
        "    def _input_fn():\n",
        "        def decode_csv(row):\n",
        "            # row is a string tensor containing the contents of one row\n",
        "            features = tf.decode_csv(row, record_defaults=DEFAULTS)  # string tensor -> list of 50 rank 0 float tensors\n",
        "            label = features.pop()  # remove last feature and use as label\n",
        "            features = tf.stack(features)  # list of rank 0 tensors -> single rank 1 tensor\n",
        "            return {TIMESERIES_COL: features}, label\n",
        "\n",
        "        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
        "        dataset = tf.data.Dataset.list_files(filename)\n",
        "        # Read in data from files\n",
        "        dataset = dataset.flat_map(tf.data.TextLineDataset)\n",
        "        # Parse text lines as comma-separated values (CSV)\n",
        "        dataset = dataset.map(decode_csv)\n",
        "\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            num_epochs = None  # loop indefinitely\n",
        "            dataset = dataset.shuffle(buffer_size=10 * batch_size)\n",
        "        else:\n",
        "            num_epochs = 1  # end-of-input after this\n",
        "\n",
        "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
        "        return dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "    return _input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eg3H9IqURH-E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def serving_input_fn():\n",
        "    feature_placeholders = {\n",
        "        TIMESERIES_COL: tf.placeholder(tf.float32, [None, N_INPUTS])\n",
        "    }\n",
        "\n",
        "    features = {\n",
        "        key: tf.expand_dims(tensor, -1)\n",
        "        for key, tensor in feature_placeholders.items()\n",
        "    }\n",
        "    features[TIMESERIES_COL] = tf.squeeze(features[TIMESERIES_COL], axis=[2])\n",
        "\n",
        "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nb4dc0SmRMoI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_errors(features, labels, predictions):\n",
        "    labels = tf.expand_dims(labels, -1)  # rank 1 -> rank 2 to match rank of predictions\n",
        "\n",
        "    if predictions.shape[1] == 1:\n",
        "        loss = tf.losses.mean_squared_error(labels, predictions)\n",
        "        rmse = tf.metrics.root_mean_squared_error(labels, predictions)\n",
        "        return loss, rmse\n",
        "    else:\n",
        "        # one prediction for every input in sequence\n",
        "        # get 1-N of (x + label)\n",
        "        labelsN = tf.concat([features[TIMESERIES_COL], labels], axis=1)\n",
        "        labelsN = labelsN[:, 1:]\n",
        "        # loss is computed from the last 1/3 of the series\n",
        "        N = (2 * N_INPUTS) // 3\n",
        "        loss = tf.losses.mean_squared_error(labelsN[:, N:], predictions[:, N:])\n",
        "        # rmse is computed from last prediction and last label\n",
        "        lastPred = predictions[:, -1]\n",
        "        rmse = tf.metrics.root_mean_squared_error(labels, lastPred)\n",
        "        return loss, rmse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3RqHjBIiRRLv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# RMSE when predicting same as last value\n",
        "def same_as_last_benchmark(features, labels):\n",
        "    predictions = features[TIMESERIES_COL][:,-1] # last value in input sequence\n",
        "    return tf.metrics.root_mean_squared_error(labels, predictions)\n",
        "\n",
        "\n",
        "# create the inference model\n",
        "def sequence_regressor(features, labels, mode, params):\n",
        "    # 1. run the appropriate model\n",
        "    model_functions = {\n",
        "        'linear': linear_model,\n",
        "        'dnn': dnn_model,\n",
        "        'cnn': cnn_model,\n",
        "        'rnn': rnn_model,\n",
        "        'rnn2': rnn2_model,\n",
        "        'rnnN': rnnN_model}\n",
        "    model_function = model_functions[params['model']]\n",
        "    predictions = model_function(features, mode, params)\n",
        "\n",
        "    # 2. loss function, training/eval ops\n",
        "    loss = None\n",
        "    train_op = None\n",
        "    eval_metric_ops = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        loss, rmse = compute_errors(features, labels, predictions)\n",
        "\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            # this is needed for batch normalization, but has no effect otherwise\n",
        "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "            with tf.control_dependencies(update_ops):\n",
        "                # 2b. set up training operation\n",
        "                train_op = tf.contrib.layers.optimize_loss(\n",
        "                    loss,\n",
        "                    tf.train.get_global_step(),\n",
        "                    learning_rate=params['learning_rate'],\n",
        "                    optimizer=\"Adam\")\n",
        "\n",
        "        # 2c. eval metric\n",
        "        eval_metric_ops = {\n",
        "            \"RMSE\": rmse,\n",
        "            \"RMSE_same_as_last\": same_as_last_benchmark(features, labels),\n",
        "        }\n",
        "\n",
        "    # 3. Create predictions\n",
        "    if predictions.shape[1] != 1:\n",
        "        predictions = predictions[:, -1]  # last predicted value\n",
        "    predictions_dict = {\"predicted\": predictions}\n",
        "\n",
        "    # 4. return EstimatorSpec\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        mode=mode,\n",
        "        predictions=predictions_dict,\n",
        "        loss=loss,\n",
        "        train_op=train_op,\n",
        "        eval_metric_ops=eval_metric_ops,\n",
        "        export_outputs={\n",
        "            'predictions': tf.estimator.export.PredictOutput(predictions_dict)}\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qng51S1QRYqe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(output_dir, hparams):\n",
        "    #get_train = read_dataset(hparams['train_data_path'],\n",
        "    #                         tf.estimator.ModeKeys.TRAIN,\n",
        "    #                         hparams['train_batch_size'])\n",
        "    #get_valid = read_dataset(hparams['eval_data_path'],\n",
        "    #                         tf.estimator.ModeKeys.EVAL,\n",
        "    #                         1000)\n",
        "    \n",
        "    get_train = _train_fn(features=features_train,\n",
        "                                    labels=labels_train,\n",
        "                                    batch_size=10)\n",
        "    \n",
        "    get_valid = _test_fn(features=features_test, \n",
        "                                      labels=labels_test,\n",
        "                                      batch_size=10)\n",
        "    \n",
        "    estimator = tf.estimator.Estimator(model_fn=sequence_regressor,\n",
        "                                       params=hparams,\n",
        "                                       config=tf.estimator.RunConfig(\n",
        "                                           save_checkpoints_secs=\n",
        "                                           hparams['min_eval_frequency']),\n",
        "                                       model_dir=output_dir)\n",
        "    train_spec = tf.estimator.TrainSpec(input_fn=get_train,\n",
        "                                        max_steps=hparams['train_steps'])\n",
        "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
        "    eval_spec = tf.estimator.EvalSpec(input_fn=get_valid,\n",
        "                                      steps=None,\n",
        "                                      exporters=exporter,\n",
        "                                      start_delay_secs=hparams['eval_delay_secs'],\n",
        "                                      throttle_secs=hparams['min_eval_frequency'])\n",
        "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vBpeXVfbRcre",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO : Check original example for shape of the data\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "EVAL_INTERVAL = 300\n",
        "TRAIN_STEPS = 10000\n",
        "EVAL_DELAY_SECS = 60\n",
        "ROOT_DIR = '/home/mmistroni/tf_logs/run-{}'\n",
        "hparams = dict(min_eval_frequency=EVAL_INTERVAL, train_steps=TRAIN_STEPS, eval_delay_secs = EVAL_DELAY_SECS, model='dnn')\n",
        "from datetime import datetime\n",
        "train_and_evaluate('/home/mmistroni/tf_logs/timeseries/run-{}-{}'.format(ROOT_DIR.format(datetime.utcnow().strftime('%Y%m%d%H%M%S')),\n",
        "                                                                         hparams['model']), hparams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e8SxyWVIxS3L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}