{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFTimeSeries.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmistroni/TensorFlowPlayground/blob/master/TFTimeSeries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "03Y2LC7ZQtOu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "9ec8aedc-c179-4344-e7c3-8c32a402b390"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "TIMESERIES_COL = 'Close'\n",
        "N_OUTPUTS = 1  # in each sequence, 1-9 are features, and 10 is label\n",
        "SEQ_LEN = 10\n",
        "DEFAULTS = 0.0\n",
        "N_INPUTS = 9\n",
        "\n",
        "def create_time_series():\n",
        "  freq = (np.random.random()*0.5) + 0.1  # 0.1 to 0.6\n",
        "  ampl = np.random.random() + 0.5  # 0.5 to 1.5\n",
        "  x = np.sin(np.arange(0,SEQ_LEN) * freq) * ampl\n",
        "  return x\n",
        "\n",
        "all_timeseries = [create_time_series() for i in range(0, SEQ_LEN * 4)]\n",
        "\n",
        "# We need to stack X numpy array on top of each other and then create a dictionary\n",
        "# for every features\n",
        "column_names = [str(idx) for idx in range(0, SEQ_LEN)]\n",
        "\n",
        "feature_names = column_names[0:-3]\n",
        "labels = column_names[-3:]\n",
        "all_data = np.stack(all_timeseries)\n",
        "\n",
        "print('All data shape is{0}'.format(all_data.shape))\n",
        "X, y = all_data[...,0:-3], all_data[...,-3:]\n",
        "\n",
        "print ('X is fo type {0}, y  of type {1}'.format(type(X), type(y)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=1)\n",
        "\n",
        "'''\n",
        "features_train = dict((fn,X_train[:, [idx]]) for idx, fn in enumerate(feature_names))\n",
        "labels_train = y_train\n",
        "\n",
        "print ('Features train is {0}'.format(type(features_train)))\n",
        "for key, val in features_train.items():\n",
        "  print ('Key is {0} Val of type {1} and shape {2} is {3}'.format(key, type(val), val.shape, val))\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All data shape is(40, 10)\n",
            "X is fo type <class 'numpy.ndarray'>, y  of type <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfeatures_train = dict((fn,X_train[:, [idx]]) for idx, fn in enumerate(feature_names))\\nlabels_train = y_train\\n\\nprint ('Features train is {0}'.format(type(features_train)))\\nfor key, val in features_train.items():\\n  print ('Key is {0} Val of type {1} and shape {2} is {3}'.format(key, type(val), val.shape, val))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "prI5nJS_S4nf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Installing libraries and downloading data as DataFrame\n"
      ]
    },
    {
      "metadata": {
        "id": "zwQN5CmUS_pS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "57d455cc-65a7-4d83-891a-0b03d3ba9d79"
      },
      "cell_type": "code",
      "source": [
        "!pip install pandas_datareader"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pandas_datareader\n",
            "  Using cached https://files.pythonhosted.org/packages/cc/5c/ea5b6dcfd0f55c5fb1e37fb45335ec01cceca199b8a79339137f5ed269e0/pandas_datareader-0.7.0-py2.py3-none-any.whl\n",
            "Collecting lxml (from pandas_datareader)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/a4/9eea8035fc7c7670e5eab97f34ff2ef0ddd78a491bf96df5accedb0e63f5/lxml-4.2.5-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.8MB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from pandas_datareader) (1.10.11)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from pandas_datareader) (2.18.4)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.6/dist-packages (from pandas_datareader) (0.22.0)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas_datareader) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas_datareader) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas_datareader) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas_datareader) (2018.10.15)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.2->pandas_datareader) (2018.7)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.2->pandas_datareader) (1.14.6)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.2->pandas_datareader) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas>=0.19.2->pandas_datareader) (1.11.0)\n",
            "Installing collected packages: lxml, pandas-datareader\n",
            "Successfully installed lxml-4.2.5 pandas-datareader-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZFt1ZH-GTPV-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas_datareader as pdr\n",
        "from datetime import date, timedelta\n",
        "startdate = date.today() - timedelta(days=100)\n",
        "stock_data = pdr.get_data_yahoo('^GSPC', startdate, date.today())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rIAYARAcTXzf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creating 10 batches of 10 Items"
      ]
    },
    {
      "metadata": {
        "id": "QeBP_1ykTgeP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "closes = stock_data[['Close']]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WafXje98Qyiq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def init(hparams):\n",
        "    global SEQ_LEN, DEFAULTS, N_INPUTS\n",
        "    N_OUTPUTS = 1  # in each sequence, 1-9 are features, and 10 is label\n",
        "    SEQ_LEN = 10\n",
        "    DEFAULTS = 0.0\n",
        "    DEFAULTS = [[0.0] for x in range(0, SEQ_LEN)]\n",
        "    N_INPUTS = SEQ_LEN - N_OUTPUTS\n",
        "\n",
        "\n",
        "def linear_model(features, mode, params):\n",
        "    X = features[TIMESERIES_COL]\n",
        "    predictions = tf.layers.dense(X, 1, activation=None)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def dnn_model(features, mode, params):\n",
        "    X = features[TIMESERIES_COL]\n",
        "    h1 = tf.layers.dense(X, 10, activation=tf.nn.relu)\n",
        "    h2 = tf.layers.dense(h1, 3, activation=tf.nn.relu)\n",
        "    predictions = tf.layers.dense(h2, 1, activation=None)  # linear output: regression\n",
        "    return predictions\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t3E-or7HQ2RF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cnn_model(features, mode, params):\n",
        "    X = tf.reshape(features[TIMESERIES_COL],\n",
        "                   [-1, N_INPUTS, 1])  # as a 1D \"sequence\" with only one time-series observation (height)\n",
        "    c1 = tf.layers.conv1d(X, filters=N_INPUTS // 2,\n",
        "                          kernel_size=3, strides=1,\n",
        "                          padding='same', activation=tf.nn.relu)\n",
        "    p1 = tf.layers.max_pooling1d(c1, pool_size=2, strides=2)\n",
        "\n",
        "    c2 = tf.layers.conv1d(p1, filters=N_INPUTS // 2,\n",
        "                          kernel_size=3, strides=1,\n",
        "                          padding='same', activation=tf.nn.relu)\n",
        "    p2 = tf.layers.max_pooling1d(c2, pool_size=2, strides=2)\n",
        "\n",
        "    outlen = p2.shape[1] * p2.shape[2]\n",
        "    c2flat = tf.reshape(p2, [-1, outlen])\n",
        "    h1 = tf.layers.dense(c2flat, 3, activation=tf.nn.relu)\n",
        "    predictions = tf.layers.dense(h1, 1, activation=None)  # linear output: regression\n",
        "    return predictions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "01uU1JjqQ5tC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rnn_model(features, mode, params):\n",
        "    CELL_SIZE = N_INPUTS // 3  # size of the internal state in each of the cells\n",
        "\n",
        "    # 1. dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
        "    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
        "\n",
        "    # 2. configure the RNN\n",
        "    cell = tf.nn.rnn_cell.GRUCell(CELL_SIZE)\n",
        "    outputs, state = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
        "\n",
        "    # 3. pass rnn output through a dense layer\n",
        "    h1 = tf.layers.dense(state, N_INPUTS // 2, activation=tf.nn.relu)\n",
        "    predictions = tf.layers.dense(h1, 1, activation=None)  # (?, 1)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# 2-layer RNN\n",
        "def rnn2_model(features, mode, params):\n",
        "    # dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
        "    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
        "\n",
        "    # 2. configure the RNN\n",
        "    cell1 = tf.nn.rnn_cell.GRUCell(N_INPUTS * 2)\n",
        "    cell2 = tf.nn.rnn_cell.GRUCell(N_INPUTS // 2)\n",
        "    cells = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
        "    outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
        "    # 'state' is now a tuple containing the final state of each cell layer\n",
        "    # we use state[1] below to extract the final state of the final layer\n",
        "    \n",
        "    # 3. pass rnn output through a dense layer\n",
        "    h1 = tf.layers.dense(state[1], cells.output_size // 2, activation=tf.nn.relu)\n",
        "    predictions = tf.layers.dense(h1, 1, activation=None)  # (?, 1)\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "34GMRROIQ9wJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rnnN_model(features, mode, params):\n",
        "    # dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
        "    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
        "\n",
        "    # 2. configure the RNN\n",
        "    cell1 = tf.nn.rnn_cell.GRUCell(N_INPUTS * 2)\n",
        "    cell2 = tf.nn.rnn_cell.GRUCell(N_INPUTS // 2)\n",
        "    cells = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
        "    outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
        "    # 'outputs' contains the state of the final layer for every time step\n",
        "    # not just the last time step (?,N_INPUTS, final cell size)\n",
        "    \n",
        "    # 3. pass state for each time step through a DNN, to get a prediction\n",
        "    # for each time step \n",
        "    h1 = tf.layers.dense(outputs, cells.output_size, activation=tf.nn.relu)\n",
        "    h2 = tf.layers.dense(h1, cells.output_size // 2, activation=tf.nn.relu)\n",
        "    predictions = tf.layers.dense(h2, 1, activation=None)  # (?, N_INPUTS, 1)\n",
        "    predictions = tf.reshape(predictions, [-1, N_INPUTS])\n",
        "    return predictions # return prediction for each time step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zDTmg52wRDTO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# read data and convert to needed format\n",
        "def read_dataset(filename, mode, batch_size=10):\n",
        "    # What we  need to do here is to somehow pass a 10*10 dataset. and we should not use decode_csv but rather do similar\n",
        "    # to what we have done in the california dtaset\n",
        "    def _input_fn():\n",
        "        def decode_csv(row):\n",
        "            # row is a string tensor containing the contents of one row\n",
        "            features = tf.decode_csv(row, record_defaults=DEFAULTS)  # string tensor -> list of 50 rank 0 float tensors\n",
        "            label = features.pop()  # remove last feature and use as label\n",
        "            features = tf.stack(features)  # list of rank 0 tensors -> single rank 1 tensor\n",
        "            return {TIMESERIES_COL: features}, label\n",
        "\n",
        "        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
        "        dataset = tf.data.Dataset.list_files(filename)\n",
        "        # Read in data from files\n",
        "        dataset = dataset.flat_map(tf.data.TextLineDataset)\n",
        "        # Parse text lines as comma-separated values (CSV)\n",
        "        dataset = dataset.map(decode_csv)\n",
        "\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            num_epochs = None  # loop indefinitely\n",
        "            dataset = dataset.shuffle(buffer_size=10 * batch_size)\n",
        "        else:\n",
        "            num_epochs = 1  # end-of-input after this\n",
        "\n",
        "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
        "        return dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "    return _input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eg3H9IqURH-E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def serving_input_fn():\n",
        "    feature_placeholders = {\n",
        "        TIMESERIES_COL: tf.placeholder(tf.float32, [None, N_INPUTS])\n",
        "    }\n",
        "\n",
        "    features = {\n",
        "        key: tf.expand_dims(tensor, -1)\n",
        "        for key, tensor in feature_placeholders.items()\n",
        "    }\n",
        "    features[TIMESERIES_COL] = tf.squeeze(features[TIMESERIES_COL], axis=[2])\n",
        "\n",
        "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nb4dc0SmRMoI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_errors(features, labels, predictions):\n",
        "    labels = tf.expand_dims(labels, -1)  # rank 1 -> rank 2 to match rank of predictions\n",
        "\n",
        "    if predictions.shape[1] == 1:\n",
        "        loss = tf.losses.mean_squared_error(labels, predictions)\n",
        "        rmse = tf.metrics.root_mean_squared_error(labels, predictions)\n",
        "        return loss, rmse\n",
        "    else:\n",
        "        # one prediction for every input in sequence\n",
        "        # get 1-N of (x + label)\n",
        "        labelsN = tf.concat([features[TIMESERIES_COL], labels], axis=1)\n",
        "        labelsN = labelsN[:, 1:]\n",
        "        # loss is computed from the last 1/3 of the series\n",
        "        N = (2 * N_INPUTS) // 3\n",
        "        loss = tf.losses.mean_squared_error(labelsN[:, N:], predictions[:, N:])\n",
        "        # rmse is computed from last prediction and last label\n",
        "        lastPred = predictions[:, -1]\n",
        "        rmse = tf.metrics.root_mean_squared_error(labels, lastPred)\n",
        "        return loss, rmse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3RqHjBIiRRLv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# RMSE when predicting same as last value\n",
        "def same_as_last_benchmark(features, labels):\n",
        "    predictions = features[TIMESERIES_COL][:,-1] # last value in input sequence\n",
        "    return tf.metrics.root_mean_squared_error(labels, predictions)\n",
        "\n",
        "\n",
        "# create the inference model\n",
        "def sequence_regressor(features, labels, mode, params):\n",
        "    # 1. run the appropriate model\n",
        "    model_functions = {\n",
        "        'linear': linear_model,\n",
        "        'dnn': dnn_model,\n",
        "        'cnn': cnn_model,\n",
        "        'rnn': rnn_model,\n",
        "        'rnn2': rnn2_model,\n",
        "        'rnnN': rnnN_model}\n",
        "    model_function = model_functions[params['model']]\n",
        "    predictions = model_function(features, mode, params)\n",
        "\n",
        "    # 2. loss function, training/eval ops\n",
        "    loss = None\n",
        "    train_op = None\n",
        "    eval_metric_ops = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        loss, rmse = compute_errors(features, labels, predictions)\n",
        "\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            # this is needed for batch normalization, but has no effect otherwise\n",
        "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "            with tf.control_dependencies(update_ops):\n",
        "                # 2b. set up training operation\n",
        "                train_op = tf.contrib.layers.optimize_loss(\n",
        "                    loss,\n",
        "                    tf.train.get_global_step(),\n",
        "                    learning_rate=params['learning_rate'],\n",
        "                    optimizer=\"Adam\")\n",
        "\n",
        "        # 2c. eval metric\n",
        "        eval_metric_ops = {\n",
        "            \"RMSE\": rmse,\n",
        "            \"RMSE_same_as_last\": same_as_last_benchmark(features, labels),\n",
        "        }\n",
        "\n",
        "    # 3. Create predictions\n",
        "    if predictions.shape[1] != 1:\n",
        "        predictions = predictions[:, -1]  # last predicted value\n",
        "    predictions_dict = {\"predicted\": predictions}\n",
        "\n",
        "    # 4. return EstimatorSpec\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        mode=mode,\n",
        "        predictions=predictions_dict,\n",
        "        loss=loss,\n",
        "        train_op=train_op,\n",
        "        eval_metric_ops=eval_metric_ops,\n",
        "        export_outputs={\n",
        "            'predictions': tf.estimator.export.PredictOutput(predictions_dict)}\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qng51S1QRYqe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(output_dir, hparams):\n",
        "    get_train = read_dataset(hparams['train_data_path'],\n",
        "                             tf.estimator.ModeKeys.TRAIN,\n",
        "                             hparams['train_batch_size'])\n",
        "    get_valid = read_dataset(hparams['eval_data_path'],\n",
        "                             tf.estimator.ModeKeys.EVAL,\n",
        "                             1000)\n",
        "    estimator = tf.estimator.Estimator(model_fn=sequence_regressor,\n",
        "                                       params=hparams,\n",
        "                                       config=tf.estimator.RunConfig(\n",
        "                                           save_checkpoints_secs=\n",
        "                                           hparams['min_eval_frequency']),\n",
        "                                       model_dir=output_dir)\n",
        "    train_spec = tf.estimator.TrainSpec(input_fn=get_train,\n",
        "                                        max_steps=hparams['train_steps'])\n",
        "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
        "    eval_spec = tf.estimator.EvalSpec(input_fn=get_valid,\n",
        "                                      steps=None,\n",
        "                                      exporters=exporter,\n",
        "                                      start_delay_secs=hparams['eval_delay_secs'],\n",
        "                                      throttle_secs=hparams['min_eval_frequency'])\n",
        "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vBpeXVfbRcre",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}