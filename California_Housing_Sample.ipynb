{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "California_Housing_Sample.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmistroni/TensorFlowPlayground/blob/master/California_Housing_Sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vh5k7rCMRp_v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import os\n",
        "import urllib\n",
        "import tarfile\n",
        "from datetime import datetime\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_AuJ8BmDRqAa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
        "HOUSING_PATH = \"datasets/housing\"\n",
        "HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + \"/housing.tgz\"\n",
        "\n",
        "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
        "    # No Luck, \n",
        "    # Instead we need to\n",
        "    # 1. download original dataset from book repo\n",
        "    if not os.path.isdir(housing_path):\n",
        "        os.makedirs(housing_path)\n",
        "        \n",
        "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
        "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
        "    housing_tgz = tarfile.open(tgz_path)\n",
        "    housing_tgz.extractall(path=housing_path)\n",
        "    housing_tgz.close()\n",
        "    \n",
        "    # 2. create a pipeline to clean up the data\n",
        "    # 3. feed the data to tensor flow\n",
        "    \n",
        "def load_housing_data(housing_path=HOUSING_PATH):\n",
        "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
        "    return pd.read_csv(csv_path)\n",
        "    \n",
        "def transform_using_pipeline(housing):\n",
        "    from sklearn.base import BaseEstimator,TransformerMixin\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.preprocessing import StandardScaler, LabelBinarizer, Imputer\n",
        "    from sklearn.pipeline import FeatureUnion\n",
        "    # column index\n",
        "    rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
        "\n",
        "    class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
        "        def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
        "            self.add_bedrooms_per_room = add_bedrooms_per_room\n",
        "        def fit(self, X, y=None):\n",
        "            return self  # nothing else to do\n",
        "        def transform(self, X, y=None):\n",
        "            rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
        "            population_per_household = X[:, population_ix] / X[:, household_ix]\n",
        "            if self.add_bedrooms_per_room:\n",
        "                bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
        "                return np.c_[X, rooms_per_household, population_per_household,\n",
        "                             bedrooms_per_room]\n",
        "            else:\n",
        "                return np.c_[X, rooms_per_household, population_per_household]\n",
        "\n",
        "            \n",
        "    \n",
        "    \n",
        "    class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
        "        def __init__(self, attribute_names):\n",
        "            self.attribute_names = attribute_names\n",
        "        def fit(self, X,  y=None):\n",
        "            return self\n",
        "        def transform(self, X):\n",
        "            return X[self.attribute_names].values\n",
        "    \n",
        "    print (type(housing))\n",
        "    housing_num = housing.drop('ocean_proximity', axis=1)\n",
        "    num_attribs = list(housing_num.columns)\n",
        "    cat_attribs = [\"ocean_proximity\"]\n",
        "    \n",
        "    num_pipeline = Pipeline([\n",
        "        ('selector', DataFrameSelector(num_attribs)),\n",
        "        ('imputer', Imputer(strategy=\"median\")),\n",
        "        ('attribs_adder', CombinedAttributesAdder()),\n",
        "        ('std_scaler', StandardScaler())\n",
        "         ])\n",
        "                             \n",
        "    cat_pipeline = Pipeline([\n",
        "        ('selector', DataFrameSelector(cat_attribs)),\n",
        "        ('label_binarizer', LabelBinarizer())\n",
        "                        ])\n",
        "    \n",
        "    full_pipeline = FeatureUnion(transformer_list=[\n",
        "            (\"num_pipeline\", num_pipeline),\n",
        "            (\"cat_pipeline\", cat_pipeline)\n",
        "        ])\n",
        "    return num_pipeline.fit_transform(housing)\n",
        "    \n",
        "    \n",
        "    \n",
        "#fetch_housing_data()\n",
        "\n",
        "def get_housing_data_as_dataframe():\n",
        "    housing = load_housing_data()\n",
        "    print ('out of here')\n",
        "    housing.info()\n",
        "    housing_prepared = transform_using_pipeline(housing)\n",
        "    return housing_prepared\n",
        "#type(hous)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "wNqwdf7QRqAr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h3>Sample Tensorflow on Housing data</h3>"
      ]
    },
    {
      "metadata": {
        "id": "ZG1rqjdjRqAz",
        "colab_type": "code",
        "colab": {},
        "outputId": "184144a3-f07d-4cdc-9fcf-d42563231f2a"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "m, n  = housing.data.shape\n",
        "housing_data_plus_bias = np.c_[np.ones((m,1)), housing.data]\n",
        "\n",
        "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
        "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
        "XT = tf.transpose(X)\n",
        "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    theta_value = theta.eval()\n",
        "    print (theta_value)\n",
        "print ('OUt of Here. ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ -3.71851807e+01]\n",
            " [  4.36337471e-01]\n",
            " [  9.39523336e-03]\n",
            " [ -1.07113101e-01]\n",
            " [  6.44792199e-01]\n",
            " [ -4.03380000e-06]\n",
            " [ -3.78137082e-03]\n",
            " [ -4.23484027e-01]\n",
            " [ -4.37219113e-01]]\n",
            "OUt of Here. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "oDB7XwlDRqBP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> TensorFlow with manual Gradient Descent </h2>"
      ]
    },
    {
      "metadata": {
        "id": "E0lHpAxvRqBU",
        "colab_type": "code",
        "colab": {},
        "outputId": "1c57a8d7-c22c-4bf2-8cba-de387e5a20fa"
      },
      "cell_type": "code",
      "source": [
        "# scaling data first\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "\n",
        "def get_optimizer(learning_rate, gradient=True):\n",
        "    if gradient:\n",
        "        return tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
        "    return tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
        "\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "m, n  = housing.data.shape\n",
        "\n",
        "# Scaling data, to improve performance\n",
        "scaler = preprocessing.StandardScaler()\n",
        "scaled_housing_data = scaler.fit_transform(housing.data)\n",
        "scaled_housing_data_plus_bias = np.c_[np.ones((m,1)), scaled_housing_data]\n",
        "\n",
        "n_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "\n",
        "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
        "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
        "theta = tf.Variable(tf.random_uniform([n+1,1], -1.0, 1.0), name=\"theta\")\n",
        "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
        "error = y_pred - y\n",
        "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
        "#gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
        "# using autodiff\n",
        "#gradients = tf.gradients(mse, [theta])[0]\n",
        "#training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
        "\n",
        "#Using optimizer\n",
        "optimizer = get_optimizer(learning_rate, False)\n",
        "training_op = optimizer.minimize(mse)\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(n_epochs):\n",
        "        if epoch % 100 == 0:\n",
        "            print (\"Epoch\", epoch, \"mse = \", mse.eval())\n",
        "        sess.run(training_op)\n",
        "    best_theta = theta.eval()\n",
        "    \n",
        "    print ('Best Theta:%s' % best_theta)\n",
        "        \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 mse =  8.18334\n",
            "Epoch 100 mse =  0.535337\n",
            "Epoch 200 mse =  0.525473\n",
            "Epoch 300 mse =  0.524469\n",
            "Epoch 400 mse =  0.524341\n",
            "Epoch 500 mse =  0.524323\n",
            "Epoch 600 mse =  0.524321\n",
            "Epoch 700 mse =  0.524321\n",
            "Epoch 800 mse =  0.524321\n",
            "Epoch 900 mse =  0.524321\n",
            "Best Theta:[[ 2.06855774]\n",
            " [ 0.82963133]\n",
            " [ 0.11875387]\n",
            " [-0.26554942]\n",
            " [ 0.30571482]\n",
            " [-0.00450235]\n",
            " [-0.03932671]\n",
            " [-0.89985853]\n",
            " [-0.87051523]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "Ebiu5-CaRqBn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Feeding Data to Algorithm (Using Batch Gradient Descent</h2>"
      ]
    },
    {
      "metadata": {
        "id": "sonj2YfoRqBx",
        "colab_type": "code",
        "colab": {},
        "outputId": "ccb7103c-3095-41f0-d964-10d35eb10efe"
      },
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "# Adding TensorBoard\n",
        "from datetime import datetime\n",
        "now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
        "root_logdir = '/home/mmistroni/tf_logs'\n",
        "logdir = '{}/run-{}'.format(root_logdir, now)\n",
        "\n",
        "def get_optimizer(learning_rate, gradient=True):\n",
        "    if gradient:\n",
        "        return tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
        "    return tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
        "\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "m, n  = housing.data.shape\n",
        "\n",
        "# Scaling data, to improve performance\n",
        "scaler = preprocessing.StandardScaler()\n",
        "scaled_housing_data = scaler.fit_transform(housing.data)\n",
        "scaled_housing_data_plus_bias = np.c_[np.ones((m,1)), scaled_housing_data]\n",
        "\n",
        "# \n",
        "batch_size = 1000\n",
        "n_batches = int(np.ceil(m / batch_size))\n",
        "\n",
        "def fetch_batch(epoch, batch_index, batch_size):\n",
        "    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book\n",
        "    indices = np.random.randint(m, size=batch_size)  # not shown\n",
        "    X_batch = scaled_housing_data_plus_bias[indices] # not shown\n",
        "    y_batch = housing.target.reshape(-1, 1)[indices] # not shown\n",
        "    return X_batch, y_batch\n",
        "\n",
        "n_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\")\n",
        "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
        "\n",
        "\n",
        "theta = tf.Variable(tf.random_uniform([n+1,1], -1.0, 1.0), name=\"theta\")\n",
        "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
        "error = y_pred - y\n",
        "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
        "\n",
        "optimizer = get_optimizer(learning_rate, False)\n",
        "training_op = optimizer.minimize(mse)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver() # Saving data\n",
        "mse_summary = tf.summary.scalar('MSE', mse)\n",
        "\n",
        "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(n_epochs):\n",
        "        for batch_index in range(n_batches):\n",
        "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
        "            if batch_index % 10 == 0:\n",
        "                summary_str = mse_summary.eval(feed_dict={X:X_batch, y:y_batch})\n",
        "                step = epoch * n_batches + batch_index\n",
        "                file_writer.add_summary(summary_str, step)\n",
        "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
        "    best_theta = theta.eval()\n",
        "    print ('Best Theta:%s' % best_theta)\n",
        "    \n",
        "    save_path = saver.save(sess, '/home/mmistroni/tf_model/my_model_final.ckpt')\n",
        "    print ('Data saved to:%s' % save_path)\n",
        "file_writer.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Theta:[[ 2.07177997]\n",
            " [ 0.85556298]\n",
            " [ 0.10774077]\n",
            " [-0.29117864]\n",
            " [ 0.3478246 ]\n",
            " [-0.00871286]\n",
            " [-0.03593429]\n",
            " [-0.89287072]\n",
            " [-0.88272148]]\n",
            "Data saved to:/home/mmistroni/tf_model/my_model_final.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "SMpEO-eYRqCQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Run same algorithm using tf.estimator </h2>\n"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "NaEcZGxrRqCW",
        "colab_type": "code",
        "colab": {},
        "outputId": "c2274ebc-e505-4f1f-9df0-2e021a21085c"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from sklearn import preprocessing\n",
        "from datetime import datetime\n",
        "print(tf.__version__)\n",
        "TRAIN_STEPS = 10000\n",
        "PRICE_NORM_FACTOR=1000\n",
        "EVAL_INTERVAL = 300\n",
        "ROOT_DIR = '/home/mmistroni/tf_logs/run-{}'\n",
        "#2.1 Next STep: Create only one function to handle Train and Evaluate\n",
        "#2.2 Create features out of csv data\n",
        "#3.Learn how to interpret TensorBoard\n",
        "#3.1 Find out why there is no output\n",
        "\n",
        "\n",
        "# Creating a TrainFn and a TestFn\n",
        "def _train_fn(features, labels, batch_size):\n",
        "    \n",
        "    def _train():\n",
        "        \"\"\"An input function for training\"\"\"\n",
        "        # Convert the inputs to a Dataset.\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "\n",
        "        # Shuffle, repeat, and batch the examples.\n",
        "        dataset = dataset.repeat(None).batch(batch_size)\n",
        "        # This will now return batches of features, label\n",
        "        return dataset.make_one_shot_iterator().get_next()\n",
        "    return _train\n",
        "\n",
        "def _test_fn(features, labels, batch_size):\n",
        "    def _test():\n",
        "        \"\"\"An input function for training\"\"\"\n",
        "        # Convert the inputs to a Dataset.\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "        # Shuffle, repeat, and batch the examples.\n",
        "        dataset = dataset.repeat(1).batch(batch_size)\n",
        "        # This will now return batches of features, label\n",
        "        return dataset.make_one_shot_iterator().get_next()\n",
        "    return _test\n",
        "\n",
        "def get_estimator(feature_cols, run_config, output_dir, linear=True):\n",
        "    if bool(linear):\n",
        "        return tf.estimator.LinearRegressor(feature_columns=feature_cols,\n",
        "                                             config=run_config,\n",
        "                                             model_dir=output_dir)\n",
        "    return tf.estimator.DNNRegressor(\n",
        "                       model_dir = output_dir,\n",
        "                       feature_columns = feature_cols,\n",
        "                       hidden_units = [64, 42],\n",
        "                       config = run_config)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "batch_size=100\n",
        "# Version 1. Using california housing\n",
        "cal_housing = fetch_california_housing()    #get_housing_data_as_dataframe()\n",
        "\n",
        "# split 80/20 train-test\n",
        "X_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n",
        "                                                    cal_housing.target,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=1)\n",
        "feature_names = cal_housing.feature_names\n",
        "\n",
        "\n",
        "\n",
        "print (type(X_train))\n",
        "print (X_train.dtype.names)\n",
        "# Train data\n",
        "# Loop incorrect, shape is (16512, 8)\n",
        "features_train = dict((fn,X_train[:, [idx]]) for idx, fn in enumerate(feature_names))\n",
        "labels_train = y_train\n",
        "\n",
        "# Test data\n",
        "features_test = dict((fn,X_test[:, [idx]]) for idx, fn in enumerate(feature_names))\n",
        "labels_test = y_test\n",
        "\n",
        "# Building features for TF\n",
        "feature_columns = [tf.feature_column.numeric_column(colName) for colName in feature_names]\n",
        "\n",
        "# Doint the real work\n",
        "run_config = tf.estimator.RunConfig(save_checkpoints_secs = EVAL_INTERVAL,\n",
        "                                      keep_checkpoint_max = 3)\n",
        "  \n",
        "#estimator = get_estimator(feature_columns, run_config, 'california_housing', linear=False)\n",
        "\n",
        "output_dir = ROOT_DIR.format(datetime.utcnow().strftime('%Y%m%d%H%M%S'))\n",
        "estimator = get_estimator(feature_columns, run_config, output_dir ,linear=False) \n",
        "\n",
        "tf.logging.info('Executing estimator of type %s', type(estimator))\n",
        "\n",
        "train_spec = tf.estimator.TrainSpec(\n",
        "                       input_fn=_train_fn(features=features_train,\n",
        "                                           labels=labels_train,\n",
        "                                           batch_size=100),\n",
        "                       max_steps = TRAIN_STEPS)\n",
        "\n",
        "\n",
        "# Evaluate how the model performs on data it has not yet seen.\n",
        "eval_spec = tf.estimator.EvalSpec(\n",
        "                       input_fn=_test_fn(features=features_test, \n",
        "                                               labels=labels_test,\n",
        "                                               batch_size=100),\n",
        "                       steps = None,\n",
        "                       start_delay_secs = 60, # start evaluating after N seconds\n",
        "                       throttle_secs = EVAL_INTERVAL)\n",
        " \n",
        "print ('Train and Evaluate....')\n",
        "# Training/Evaluating:\n",
        "tf.logging.set_verbosity(tf.logging.INFO) # Just to have some logs to display for demonstration\n",
        "\n",
        "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.0\n",
            "<class 'numpy.ndarray'>\n",
            "None\n",
            "Train and Evaluate....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E3RCy-hlRqC3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}